{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10598603,"sourceType":"datasetVersion","datasetId":6560091},{"sourceId":10598630,"sourceType":"datasetVersion","datasetId":6560111},{"sourceId":10598667,"sourceType":"datasetVersion","datasetId":6560143},{"sourceId":10598700,"sourceType":"datasetVersion","datasetId":6560167},{"sourceId":10598805,"sourceType":"datasetVersion","datasetId":6560244},{"sourceId":10631677,"sourceType":"datasetVersion","datasetId":6582473},{"sourceId":10631686,"sourceType":"datasetVersion","datasetId":6582478}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    num = 0\n    for filename in filenames:\n        num += 1\n        #print(os.path.join(dirname, filename))\n    print(dirname,num)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:06:50.567665Z","iopub.execute_input":"2025-02-01T01:06:50.567936Z","iopub.status.idle":"2025-02-01T01:11:15.636070Z","shell.execute_reply.started":"2025-02-01T01:06:50.567902Z","shell.execute_reply":"2025-02-01T01:11:15.635099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n    DataSet库说明-\n    读取文件路径-img_dir\n    读取50,000张图片\n    输出为 X, Y\n    X是（10000，4,56,56） 1000表示样本数 4表示通道数（包含四个emcal hcal trkn trkp)\n    Y是（10000,56,56）\n\"\"\"\n# 导入相关库\nimport os # 与系统文件交互\nimport tifffile as tiff #读取tiff文件格式\nfrom PIL import Image #图片处理\n#与torch 相关的库\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n#from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport imageio \n\nclass MaxMinNormalizeGlobalPerChannel:\n    \"\"\"\n    针对 (Batch, Channel, Width, Height) 的张量，\n    在所有 Batch 中对每个通道整体进行最大最小归一化。\n    \"\"\"\n    def __call__(self, tensor):\n        # 确保输入是 (Batch, Channel, Width, Height) 的张量\n        assert tensor.dim() == 4, \"Input tensor must have 4 dimensions: (Batch, Channel, Width, Height).\"\n        \n        # 计算每个通道在所有 Batch 上的全局最小值和最大值\n        # 结果是 (Channel, 1, 1)\n        min_vals = tensor.amin(dim=(0, 2, 3), keepdim=True)  # 在 Batch、Width、Height 维度求最小值\n        max_vals = tensor.amax(dim=(0, 2, 3), keepdim=True)  # 在 Batch、Width、Height 维度求最大值\n        \n        # 最大最小归一化公式\n        tensor = (tensor - min_vals) / (max_vals - min_vals + 1e-8)\n        \n        return tensor\n\n\n#创建数据集\nclass MyDataSet(Dataset):\n    def __init__(self,img_dir,group_size=10000,size_in=10000,transform=None,\n                split_shuffle = True,splition = True):\n        self.img_dir=img_dir\n        self.images=os.listdir(img_dir)\n        self.transform=transform\n        self.all_imgs=[]\n        self.emcal=[]\n        self.hcal=[]\n        self.trkn=[]\n        self.trkp=[]\n        self.truth=[]\n        self.group_size=group_size\n        self.size_in=size_in\n        self.splition=splition\n        self.split_shuffle = split_shuffle\n        self.load_images()\n        #self.normalize()\n    \n    def load_images(self):\n        all_imgs=[]\n        to_pil = transforms.ToPILImage()\n        prefixes = ['emcal', 'hcal', 'trkn', 'trkp', 'truth']\n        for prefix in prefixes:\n            for i in range(self.size_in):\n                filename = f\"{prefix}_{str(i)}.tiff\"\n                img_path = img_path=os.path.join(self.img_dir, filename)\n                #print(img_path)\n                img_array=tiff.imread(img_path)\n                img=Image.fromarray(img_array)\n                img_tensor=transform(img)\n                all_imgs.append(img_tensor)\n        self.emcal=all_imgs[:self.size_in]\n        self.hcal=all_imgs[self.group_size:self.group_size+self.size_in]\n        self.trkn=all_imgs[2*self.group_size:2*self.group_size+self.size_in]\n        self.trkp=all_imgs[3*self.group_size:3*self.group_size+self.size_in]\n        self.truth=all_imgs[4*self.group_size:4*self.group_size+self.size_in]\n        \n        self.X=[]\n        self.Y=[]\n        picture = np.ndarray([])\n        \n        if self.transform is not None:\n            transformation = self.transform\n            print('transformation is not None')\n        else:\n            transformation = lambda x: x\n            print('transformation is None')\n        \n        for emcal, hcal, trkn, trkp in zip(self.emcal,self.hcal,self.trkn, self.trkp):\n            combined_features=torch.stack((emcal,hcal,trkn,trkp),dim=0).reshape(-1,56,56)\n            self.X.append(combined_features)\n        \n        self.X=torch.stack(self.X).squeeze()\n        self.X=transformation(self.X)\n        self.Y=torch.stack(self.truth)\n        self.Y=transformation(self.Y)\n        \n        N = self.X.size(0)\n        train_size = int(0.8 * N)\n        val_size = int(0.1 * N)\n        if self.split_shuffle:\n            indices = torch.randperm(N)\n\n        else:\n            indices = torch.arange(N)\n            # 按照比例划分索引\n        train_indices = indices[:train_size]\n        val_indices = indices[train_size:train_size + val_size]\n        test_indices = indices[train_size + val_size:]\n        if self.splition == True:\n            # 根据索引划分数据集\n            self.train_X = self.X[train_indices]\n            self.train_Y = self.Y[train_indices]\n            self.val_X = self.X[val_indices]\n            self.val_Y = self.Y[val_indices]\n            self.test_X = self.X[test_indices]\n            self.test_Y = self.Y[test_indices]\n            # 释放内存\n            del self.X\n            del self.Y\n\n\n\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self,idx):\n        return self.X[idx], self.Y[idx]\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    # 数据预处理后期添加\n])\n\n    \nclass dataset_2(Dataset):\n    def __init__(self,X,Y):\n        self.X=X\n        self.Y=Y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self,idx):\n        return self.X[idx], self.Y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:11:36.633309Z","iopub.execute_input":"2025-02-01T01:11:36.633668Z","iopub.status.idle":"2025-02-01T01:11:40.974114Z","shell.execute_reply.started":"2025-02-01T01:11:36.633640Z","shell.execute_reply":"2025-02-01T01:11:40.973453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nclass CNN_5layer(nn.Module):\n    def __init__(self):\n        super(CNN_5layer,self).__init__()\n        self.cov3x3 = nn.Conv2d(4, 2, kernel_size=3, padding=1)\n        self.cov5x5 = nn.Conv2d(4, 2, kernel_size=5, padding=2)\n        self.cov7x7 = nn.Conv2d(4, 2, kernel_size=7, padding=3)\n\n        self.encoder=nn.Sequential(\n            nn.Conv2d(6, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        # self.se1=SEBlock(128)\n        self.decoder=nn.Sequential(\n            nn.Conv2d(64, 16, kernel_size=5, padding=2),\n            nn.Conv2d(16, 1, kernel_size=5, padding=2),\n            nn.Sigmoid()\n        )\n    def forward(self,x): #x=torch.cat((emcal,hcal,trkn,trkp),dim=1) (4,56,56)\n        x1=self.cov3x3(x)\n        x2=self.cov5x5(x)\n        x3=self.cov7x7(x)\n        x = torch.cat((x1,x2,x3),dim=1)\n        x=self.encoder(x)\n        x=self.decoder(x)\n        return x\n    \nclass CNN_8layer(nn.Module):\n    def __init__(self):\n        super(CNN_8layer,self).__init__()\n\n        self.encoder=nn.Sequential(\n            nn.Conv2d(4, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 56x56 -> 28x28\n            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 28x28 -> 14x14\n            nn.Conv2d(256, 512, kernel_size=5, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)   # 14x14 -> 7x7\n        )\n        self.decoder=nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),  # 7x7 -> 14x14\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),   # 14x14 -> 28x28\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 28x28 -> 56x56\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n            nn.Sigmoid()\n        )\n    def forward(self,x): #x=torch.cat((emcal,hcal,trkn,trkp),dim=1) (4,56,56)\n        x=self.encoder(x)\n        x=self.decoder(x)\n        return x\n\nclass CNN3D(nn.Module):\n    def __init__(self):\n        super(CNN3D,self).__init__()\n        self.conv3x3x3 = nn.Conv3d(1, 2, kernel_size=3, padding=(0,1,1))\n        self.conv3x5x5 = nn.Conv3d(1, 2, kernel_size=(3,5,5), padding=(0,2,2))\n        self.conv3x7x7 = nn.Conv3d(1, 2, kernel_size=(3,7,7), padding=(0,3,3))\n\n        self.encoder=nn.Sequential(\n            nn.Conv2d(12, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.decoder=nn.Sequential(\n            nn.Conv2d(64, 16, kernel_size=5, padding=2),\n            nn.Conv2d(16, 1, kernel_size=5, padding=2),\n            nn.Sigmoid()\n        )\n    def forward(self,x): #x=torch.cat((emcal,hcal,trkn,trkp),dim=1) (4,56,56)\n        x = x.unsqueeze(1)\n        x_e_h_n = x[:,:,:3,:,:]\n        x_e_h_p = x[:,:,[0,1,3],:,:]\n        x2 = self.conv3x3x3(x_e_h_n)\n        x3 = self.conv3x5x5(x_e_h_n)\n        x4 = self.conv3x7x7(x_e_h_n)\n        x5 = self.conv3x3x3(x_e_h_p)\n        x6 = self.conv3x5x5(x_e_h_p)\n        x7 = self.conv3x7x7(x_e_h_p)\n        x = torch.cat((x2,x3,x4,x5,x6,x7),dim=1).view(-1,12,56,56)\n        x=self.encoder(x)\n        x=self.decoder(x)\n        return x\n    \nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEBlock,self).__init__()\n        self.fc1=nn.Linear(channels,channels//reduction,bias=False)\n        self.fc2=nn.Linear(channels//reduction,channels,bias=False)\n\n    def forward(self,x):\n        b, c,_,_=x.size()\n        y = F.adaptive_avg_pool2d(x, (1, 1)).view(b, c) # Squeeze\n        y=F.relu(self.fc1(y))\n        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1) # Excitation - 2nd layer\n        return x * y.expand_as(x) # Scale\n\nclass CNN_with_SEBlock(nn.Module):\n    def __init__(self):\n        super(CNN_with_SEBlock,self).__init__()\n\n        self.encoder=nn.Sequential(\n            nn.Conv2d(4, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True), \n            nn.MaxPool2d(2, 2),  # 56x56 -> 28x28\n            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 28x28 -> 14x14\n            nn.Conv2d(256, 512, kernel_size=5, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)   # 14x14 -> 7x7\n        )\n        self.se1=SEBlock(512)\n        self.decoder=nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),  # 7x7 -> 14x14\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),   # 14x14 -> 28x28\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 28x28 -> 56x56\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n            nn.Sigmoid()\n        )\n    def forward(self,x): #x=torch.cat((emcal,hcal,trkn,trkp),dim=1) (4,56,56)\n        x=self.encoder(x)\n        x=self.decoder(x)\n        return x\n    \nclass Conv_UNet(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(Conv_UNet, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(C_in, C_out, kernel_size=3, stride=1, padding=1),  # 3x3卷积，padding=1保持尺寸不变\n            nn.BatchNorm2d(C_out),\n            nn.Dropout(0.3),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv2d(C_out, C_out, kernel_size=3, stride=1, padding=1),  # 再次保持尺寸不变\n            nn.BatchNorm2d(C_out),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\nclass DownSampling_UNet(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(DownSampling_UNet, self).__init__()\n        self.Down = nn.Sequential(\n            nn.Conv2d(C_in, C_out, kernel_size=2, stride=2),  # 2x2卷积，步幅2会让特征尺寸减半\n            nn.LeakyReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.Down(x)\n\nclass UpSampling_UNet(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(UpSampling_UNet, self).__init__()\n        self.Up = nn.Conv2d(C_in, C_out, kernel_size=1)  # 改变通道数的卷积\n\n    def forward(self, x, r):\n        up = F.interpolate(x, scale_factor=2, mode='nearest')  # 使用最近邻插值进行上采样\n        x = self.Up(up)  # 改变输出通道数\n        x = torch.cat([x, r], dim=1)  # 进行跳跃连接，拼接特征\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.C1 = Conv_UNet(4, 64)  # 输入图像有4个通道\n        self.D1 = DownSampling_UNet(64, 128)  # 下采样\n        self.C2 = Conv_UNet(128, 128)\n        self.D2 = DownSampling_UNet(128, 256)  # 第二次下采样\n        self.C3 = Conv_UNet(256, 256)\n        self.U1 = UpSampling_UNet(256, 128)  # 第一次上采样\n        self.C4 = Conv_UNet(256, 128)  # 拼接后通道数为256\n        self.U2 = UpSampling_UNet(128, 64)  # 第二次上采样\n        self.C5 = Conv_UNet(128, 64)  # 拼接后通道数为128\n        self.pred = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n        self.sigmoid = nn.Sigmoid()  # 对最终输出使用Sigmoid激活函数\n\n    def forward(self, x):\n        R1 = self.C1(x)  # 第1层卷积\n        R2 = self.C2(self.D1(R1))  # 下采样后卷积\n        R3 = self.C3(self.D2(R2))  # 第二次下采样后卷积\n        up1 = self.U1(R3, R2)  # 第一次上采样，并进行跳跃连接\n        up2 = self.U2(R1, up1)  # 第二次上采样，并进行跳跃连接\n\n        c = self.C5(up2)  # 最后一层卷积\n        return self.sigmoid(self.pred(c))  # 进行最后的预测并激活\n\nclass SelfAttention_UNetwithattention(nn.Module):\n    def __init__(self, in_channels):\n        super(SelfAttention_UNetwithattention, self).__init__()\n        self.query = nn.Conv2d(in_channels, in_channels, kernel_size=1) #H*W->H*W\n        self.key = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        batch_size, C, H, W = x.size()\n        # 生成查询、键、值\n        queries = self.query(x).view(batch_size, C, -1) # (B, C, H*W)\n        keys = self.key(x).view(batch_size, C, -1) # (B, C, H*W)\n        values = self.value(x).view(batch_size, C, -1) # (B, C, H*W)\n\n        # 计算自注意力\n        attention_scores = torch.bmm(queries.permute(0, 2, 1), keys) # (B, H*W, H*W)\n        attention_scores = self.softmax(attention_scores)\n\n        out = torch.bmm(values, attention_scores.permute(0, 2, 1)) # (B, C, H*W)\n        return out.view(batch_size, C, H, W) #不改变形状\n\n# 定义U-Net与Transformer结合的模型\nclass UNetwithattention(nn.Module):\n    def __init__(self, in_channels, out_channels=1): # 输出通道调整为1\n        super(UNetwithattention, self).__init__()\n        # 编码器部分\n        self.enc1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1) # 输入通道为4 \n        self.enc2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.enc3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        # 自注意力层\n        self.attention = SelfAttention_UNetwithattention(128) #输入256个通道进入\n        # 解码器部分\n        self.dec1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.conv1=nn.Conv2d(256,128,kernel_size=1)\n        self.dec2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.dec3 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1) # 最终输出通道为1\n        self.conv2=nn.Conv2d(128,128,kernel_size=3,padding=1)\n        self.conv3=nn.Conv2d(128,64,kernel_size=1)\n    def forward(self, x):\n        # 编码\n        enc1 = F.relu(self.enc1(x)) # 4*56*56->64*56*56\n        enc2 = F.relu(self.enc2(F.max_pool2d(enc1, 2))) #64*28*28->128*28*28\n        enc3 = F.relu(self.enc3(F.max_pool2d(enc2, 2))) #128*14*14->256*14*14\n        # 自注意力\n        attn_out = self.attention(enc3) # 256*14*14->256*14*14\n        # 解码\n        dec1=F.relu(self.dec1(F.upsample(attn_out,scale_factor=2,mode='bilinear',align_corners=False))) #128*28*28\n        dec1=torch.cat([dec1,enc2],dim=1) # 256*28*28\n        dec1=self.conv1(dec1)\n        dec2 = F.relu(self.dec2(F.upsample(attn_out, scale_factor=2, mode='bilinear', align_corners=False))) #128*28*28->64*56*56\n        dec3 = torch.cat([dec2, enc1], dim=1) # 跳跃连接\n        dec4=self.conv2(dec3) #B 64 56 56\n        dec5 = self.conv3(dec4)\n        out = self.dec3(dec5) #64*56*56->1*56*56\n        \n        return out\n    \n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x[:1], key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n        return x.squeeze(0)\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        def stem(x):\n            x = self.relu1(self.bn1(self.conv1(x)))\n            x = self.relu2(self.bn2(self.conv2(x)))\n            x = self.relu3(self.bn3(self.conv3(x)))\n            x = self.avgpool(x)\n            return x\n\n        x = x.type(self.conv1.weight.dtype)\n        x = stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        \n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n        embed_dim: int,\n        # vision\n        image_resolution: int,\n        vision_layers: Union[Tuple[int, int, int, int], int],\n        vision_width: int,\n        vision_patch_size: int,\n        # text\n        context_length: int,\n        vocab_size: int,\n        transformer_width: int,\n        transformer_heads: int,\n        transformer_layers: int\n        ):\n        super().__init__()\n\n        self.context_length = context_length\n\n        if isinstance(vision_layers, (tuple, list)):\n            vision_heads = vision_width * 32 // 64\n            self.visual = ModifiedResNet(\n                layers=vision_layers,\n                output_dim=embed_dim,\n                heads=vision_heads,\n                input_resolution=image_resolution,\n                width=vision_width\n            )\n        else:\n            vision_heads = vision_width // 64\n            self.visual = VisionTransformer(\n                input_resolution=image_resolution,\n                patch_size=vision_patch_size,\n                width=vision_width,\n                layers=vision_layers,\n                heads=vision_heads,\n                output_dim=embed_dim\n            )\n\n        self.transformer = Transformer(\n            width=transformer_width,\n            layers=transformer_layers,\n            heads=transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = vocab_size\n        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n        self.ln_final = LayerNorm(transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        if isinstance(self.visual, ModifiedResNet):\n            if self.visual.attnpool is not None:\n                std = self.visual.attnpool.c_proj.in_features ** -0.5\n                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n\n            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n                for name, param in resnet_block.named_parameters():\n                    if name.endswith(\"bn3.weight\"):\n                        nn.init.zeros_(param)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n\n        # cosine similarity as logits\n        logit_scale = self.logit_scale.exp()\n        #logit_scale=self.logit_scale\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n\n        # shape = [global_batch_size, global_batch_size]\n        return logits_per_image, logits_per_text\n\n\ndef convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp32(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp32)\n\n\ndef build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_resolution = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n\n    model = CLIP(\n        embed_dim,\n        image_resolution, vision_layers, vision_width, vision_patch_size,\n        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        if key in state_dict:\n            del state_dict[key]\n\n    convert_weights(model)\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\nclass easyCNN(nn.Module):\n    def __init__(self):\n        super(easyCNN,self).__init__()\n\n        self.encoder=nn.Sequential(\n            nn.Conv2d(4, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 56x56 -> 28x28\n            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),  # 28x28 -> 14x14\n            nn.Conv2d(256, 512, kernel_size=5, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)   # 14x14 -> 7x7\n        )\n        self.decoder=nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),  # 7x7 -> 14x14\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),   # 14x14 -> 28x28\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 28x28 -> 56x56\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=5, padding=2),\n            nn.Sigmoid()\n        )\n    def forward(self,x): #x=torch.cat((emcal,hcal,trkn,trkp),dim=1) (4,56,56)\n        x=self.encoder(x)\n        x=self.decoder(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:11:51.136441Z","iopub.execute_input":"2025-02-01T01:11:51.136880Z","iopub.status.idle":"2025-02-01T01:11:51.211250Z","shell.execute_reply.started":"2025-02-01T01:11:51.136858Z","shell.execute_reply":"2025-02-01T01:11:51.210412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\nimport os\n\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, loss_function):\n    model.train()\n    mean_loss = torch.zeros(1).to(device)\n    \n    data_loader = tqdm(data_loader, file=sys.stdout)\n    \n    for step, (batch_X, batch_Y) in enumerate(data_loader):\n        optimizer.zero_grad()\n        outputs=model(batch_X.to(device))\n        loss=loss_function(outputs,batch_Y.to(device))\n        loss.backward()\n        optimizer.step()\n        \n        mean_loss = (mean_loss * step + loss.detach()) / (step + 1)  # update mean losses\n        # 打印平均loss\n        data_loader.desc = \"[epoch {}] mean loss {}\".format(epoch, round(mean_loss.item(), 7))\n        \n        if not torch.isfinite(loss):\n            print('WARNING: non-finite loss, ending training ', loss)\n            sys.exit(1)\n        \n        \n    return mean_loss.item()\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device, loss_function):\n    mean_loss = torch.zeros(1).to(device)\n    model.eval()\n    val_loss = 0\n    for batch_X, batch_Y in data_loader:\n        outputs=model(batch_X.to(device))\n        mean_loss += loss_function(outputs,batch_Y.to(device)).detach()\n    mean_loss /= len(data_loader)\n    return mean_loss.item()\n\n@torch.no_grad()\ndef plot_image(net, data_loader, device, label):\n    batch_size = data_loader.batch_size\n    plot_num = min(batch_size, 5)\n    net.eval()\n    fig_list = []\n    for batch_X, batch_Y in data_loader:\n        outputs=net(batch_X.to(device)).detach()\n        for i in range(plot_num):\n            fig = plt.figure()\n            fig.suptitle(label, fontsize=16)\n            ax1 = fig.add_subplot(121)\n            ax2 = fig.add_subplot(122)\n            ax1.imshow(batch_Y[i].cpu().numpy().squeeze(),cmap='jet')\n            ax1.axis('off')\n            ax1.set_title('Ground Truth')\n            ax2.imshow(outputs[i].cpu().numpy().squeeze(),cmap='jet')\n            ax2.axis('off')\n            ax2.set_title(f'Prediction')\n            \n            fig_list.append(fig)\n        break\n    return fig_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:12:10.251169Z","iopub.execute_input":"2025-02-01T01:12:10.251545Z","iopub.status.idle":"2025-02-01T01:12:10.260682Z","shell.execute_reply.started":"2025-02-01T01:12:10.251522Z","shell.execute_reply":"2025-02-01T01:12:10.259752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport argparse\nimport random\nimport numpy as np\nimport tifffile\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\nimport torch.optim.lr_scheduler as lr_scheduler\nimport shutil\n\n#from model import CNN3D\n#from DataSet import MaxMinNormalizeGlobalPerChannel,MyDataSet, dataset_2\n#from train_and_eval import train_one_epoch, evaluate,plot_image\n\nos.environ['PYTHONHASHSEED'] = str(26)\nrandom.seed(26)\nnp.random.seed(26)\ntorch.manual_seed(26)\ntorch.cuda.manual_seed(26)\ntorch.cuda.manual_seed_all(26) \ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.use_deterministic_algorithms(True)\n\ndef train(args):\n    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n\n    print(args)\n    tb_writer = SummaryWriter(log_dir=\"runs/Demo1\")\n    if os.path.exists(\"./weights\") is False:\n        os.makedirs(\"./weights\")\n\n    # 定义训练以及预测时的预处理方法\n    data_transform = {\n        \"without_jet\": transforms.Compose([MaxMinNormalizeGlobalPerChannel()]),\n        \"jet\": transforms.Compose([MaxMinNormalizeGlobalPerChannel()])}\n\n    # 实例化训练数据集\n    data_set = MyDataSet(img_dir=args.img_dir,\n                        group_size=10000,\n                        size_in = 10000,\n                        splition = True,\n                        split_shuffle = False,\n                        transform=data_transform[\"without_jet\"])\n    train_dataset = dataset_2(data_set.train_X, data_set.train_Y)\n    val_dataset = dataset_2(data_set.val_X, data_set.val_Y)\n    test_dataset = dataset_2(data_set.test_X, data_set.test_Y)\n    data_set_jet = MyDataSet(img_dir=args.jet_dir,\n                                    group_size=1000,\n                                    size_in = 1000,\n                                    splition= False,\n                                    split_shuffle = False,\n                                    transform=data_transform[\"jet\"])\n    jet_dataset = dataset_2(data_set_jet.X, data_set_jet.Y)\n    \n    batch_size = args.batch_size\n    # 计算使用num_workers的数量\n    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 0])  # number of workers\n    print('Using {} dataloader workers every process'.format(nw))\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n                                            batch_size=batch_size,\n                                            shuffle=False,\n                                            pin_memory=True,\n                                            num_workers=nw)\n\n    val_loader = torch.utils.data.DataLoader(val_dataset,\n                                            batch_size=batch_size,\n                                            shuffle=False,\n                                            pin_memory=True,\n                                            num_workers=nw)\n    \n    test_loader = torch.utils.data.DataLoader(test_dataset,\n                                            batch_size=batch_size,\n                                            shuffle=False,\n                                            pin_memory=True,\n                                            num_workers=nw)\n    \n    jet_loader = torch.utils.data.DataLoader(jet_dataset,\n                                            batch_size=batch_size,\n                                            shuffle=False)\n    \n    # 实例化模型\n    model = CNN3D().to(device)\n\n    # 将模型写入tensorboard\n    init_img = torch.zeros((1, 4, 56, 56), device=device)\n    tb_writer.add_graph(model, init_img)\n\n    # 如果存在预训练权重则载入\n    if args.weights is None:\n        print(\"No weights file provided. Using random defaults.\")\n    else:\n        model.load_state_dict(torch.load(args.weights))\n        print(\"using pretrain-weights.\")\n\n    # 是否冻结权重\n    if args.freeze_layers:\n        print(\"freeze layers except fc layer.\")\n        for name, para in model.named_parameters():\n            # 除最后的全连接层外，其他权重全部冻结\n            if \"decoder\" not in name:\n                para.requires_grad_(False)\n        \n    warmup_epochs_1 = 40\n    warmup_epochs_2 = 80\n    warmup_epochs_3 = 83\n    learningrate = args.lr\n\n    def lf_function(epoch): \n        if epoch < warmup_epochs_1:\n            return 1\n        elif epoch < warmup_epochs_2: \n            return 0.1\n        elif epoch < warmup_epochs_3:\n            return((epoch - warmup_epochs_2) / (warmup_epochs_3 - warmup_epochs_2)) * 0.5 + 0.1\n        else:\n            return(((1 + math.cos((epoch - warmup_epochs_3) * math.pi / (args.epochs - warmup_epochs_3))) / 2) * 0.5 + 0.1)\n    optimizer = optim.Adam(model.parameters(), lr=learningrate)\n    # scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf_function)\n    loss_function = torch.nn.MSELoss()\n    \n    for epoch in range(args.epochs):\n        # train\n        train_loss = train_one_epoch(model=model,\n                                    optimizer=optimizer,\n                                    data_loader=train_loader,\n                                    device=device,\n                                    epoch=epoch,\n                                    loss_function=loss_function)\n        # update learning rate\n        scheduler.step()\n\n        # validate\n        if args.patten == \"train\":\n            test_loss = evaluate(model=model,\n                    data_loader=val_loader,\n                    device=device,\n                    loss_function=loss_function)\n        else:\n            test_loss = evaluate(model=model,\n                    data_loader=test_loader,\n                    device=device,\n                    loss_function=loss_function)\n\n        # add loss, acc and lr into tensorboard\n        print(\"[epoch {}] loss: {}\".format(epoch, round(test_loss, 7)))\n        tags = [\"train_loss\", \"test_loss\", \"learning_rate\"]\n        tb_writer.add_scalar(tags[0], train_loss, epoch)\n        tb_writer.add_scalar(tags[1], test_loss, epoch)\n        tb_writer.add_scalar(tags[2], optimizer.param_groups[0][\"lr\"], epoch)\n\n        # add figure into tensorboard\n        if (epoch + 1) % 10 == 0:\n            fig_test = plot_image(net = model, \n                                data_loader = val_loader,\n                                device = device,\n                                label = \"test\")\n            fig_jet = plot_image(net = model,\n                                data_loader = jet_loader,\n                                device = device,\n                                label = \"jet\")\n\n            if fig_test is not None:\n                tb_writer.add_figure(\"predictions without jet\",\n                                    figure=fig_test,\n                                    global_step=epoch)\n            if fig_jet is not None:\n                tb_writer.add_figure(\"predictions with jet\",\n                                    figure=fig_jet,\n                                    global_step=epoch)\n\n        # 可以添加权重的直方图,暂时不添加\n        # tb_writer.add_histogram(tag=\"\",\n        #                         values=model.conv1.weight,\n        #                         global_step=epoch)\n        # tb_writer.add_histogram(tag=\"\",\n        #                         values=model.layer1[0].conv1.weight,\n        #                         global_step=epoch)\n        \n        if ((epoch+1) % args.saving_routine == 0) or (epoch == args.epochs-1):\n            # save weights\n            torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))\n\n    num_cases_to_plot=5\n    test_samples=list(test_loader)[:num_cases_to_plot]\n    model.eval()\n    predicted_images=[]\n    true_images=[]\n    print(len(test_samples))\n    \n    with torch.no_grad():\n        for X_test, Y_test in test_samples:\n            # outputs=model(X_test.to(device),edge_index.to(device))\n            outputs=model(X_test.to(device))\n            predicted_images.append(outputs.cpu().detach().numpy())\n            true_images.append(Y_test.cpu().detach().numpy())\n    predicted_images = np.concatenate(predicted_images, axis=0)\n    true_images = np.concatenate(true_images, axis=0)\n    \n    IMAGE_NAME = 'Gauss_S1.00_NL0.30_B0.50'\n    error_list = []\n    pre_list = np.empty([])\n    true_list = np.empty([])\n    # Plotting the results\n    HEIGHT = 56\n    WIDTH =56\n    fig, axes = plt.subplots(num_cases_to_plot, 2, figsize=(10, 4 * num_cases_to_plot))\n    for i in range(num_cases_to_plot):\n        # 假设只有单一通道要显示，可以通过 denormalized_predicted_images 和 denormalized_true_images 访问真实与预测结果\n        #以下记得修改\n        pred_img = predicted_images[i].reshape(HEIGHT, WIDTH) # 假设输出是单通道形式\n        true_img = true_images[i, 0].reshape(HEIGHT, WIDTH) # 假设通道在第一维度\n        # 绘制真实图像\n        axes[i, 0].imshow(true_img) # 使用灰度图显示\n        axes[i, 0].set_title(f'True Image {i+1}')\n        axes[i, 0].axis('off')\n    \n        # 绘制预测图像\n        axes[i, 1].imshow(pred_img) # 使用灰度图显示\n        axes[i, 1].set_title(f'Predicted Image {i+1}')\n        axes[i, 1].axis('off')\n        error_list.append(pred_img-true_img)\n        pre_list = np.append(pre_list,pred_img)\n        true_list = np.append(true_list,true_img)\n    plt.show()\n\n    ''''''''''''\n    #假设已有模型,没有加载好jet数据集\n    ''''''''''''\n    data_transform = {\n    \t\"without_jet\": transforms.Compose([MaxMinNormalizeGlobalPerChannel()]),\n    \t\"jet\": transforms.Compose([MaxMinNormalizeGlobalPerChannel()])}\n    data_set_jet = MyDataSet(img_dir=args.jet_dir,\n    \t\t\t\t\t\t\t\tgroup_size=1000,\n    \t\t\t\t\t\t\t\tsize_in = 1000,\n    \t\t\t\t\t\t\t\tsplition= False,\n    \t\t\t\t\t\t\t\tsplit_shuffle = False,\n    \t\t\t\t\t\t\t\ttransform=data_transform[\"jet\"])\n    X=data_set_jet.X #(1000,4,56,56)\n    Y=data_set_jet.Y #(1000,1,56,56)\n    \n    dataset1=dataset_2(X,Y)\n    #分割数据集\n    TEST_NUM=1000\n    BATCH_SIZE=200\n    print(TEST_NUM)\n    test_loader_jet = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=False)\n    save_dir = 'predicted_images_CNN3D'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n        print(f\"文件夹 '{save_dir}' 已创建。\")\n    else:\n        shutil.rmtree(save_dir)\n        print(f\"文件夹 '{save_dir}' 及其内容已删除。\")\n        os.makedirs(save_dir)\n        print(f\"文件夹 '{save_dir}' 已重新创建。\")\n        \n    model.eval()\n    predicted_images=[]\n    \n    with torch.no_grad():\n        for i,(X_test, Y_test) in enumerate(test_loader_jet):\n            outputs=model(X_test.to(device))\n            predicted_images.append(outputs.cpu().detach().numpy())\n    predicted_images = np.concatenate(predicted_images, axis=0)\n    #print(predicted_images.shape)\n    #print(type(predicted_images))\n    for i in range(predicted_images.shape[0]):\n        # 取出第 i 张图 (形状 [1, 56, 56])\n        image_2d = predicted_images[i, 0]  # 形状 [56, 56]\n        # 转为 NumPy 数组 (默认是 float32 或 float64，具体看你的张量类型)\n        image_np = image_2d\n        # 直接写入 TIFF，不做任何缩放，保留原始精度\n        save_path = os.path.join(save_dir, f\"predict_{i}.tiff\")\n        tifffile.imwrite(save_path, image_np)\n\n    \nclass Args:\n    def __init__(self):\n        self.num_classes = 5\n        self.epochs = 100\n        self.saving_routine = 20\n        self.batch_size = 400\n        self.lr = 0.001\n        self.patten = \"Parameter\"\n        self.img_dir = '/kaggle/input/gauss-s1-00-nl0-30-b0-50'  # 修改为你的图片目录\n        self.jet_dir = '/kaggle/input/gauss-s1-00-nl0-30-b0-50-jet'    # 修改为你的Jet目录\n        self.weights = None  # 如果有预训练权重，修改为权重路径\n        self.freeze_layers = False\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nopt = Args()\ntrain(opt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:57:59.945137Z","iopub.execute_input":"2025-02-01T01:57:59.945484Z","iopub.status.idle":"2025-02-01T02:04:02.567540Z","shell.execute_reply.started":"2025-02-01T01:57:59.945457Z","shell.execute_reply":"2025-02-01T02:04:02.566669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"格式转换，将tiff的图片转化为fastjet输入需要的txt文件形式","metadata":{}},{"cell_type":"code","source":"#文件夹内1000张tiff图片转化为txt文件,此处以CNN3D为例\nimport os\nimport tifffile as tiff\nimport sys\nimport math\ndirectory_path = '/kaggle/working/predicted_images_CNN3D'\nsys.stdout=open('/kaggle/working/output_CNN3D_predict.txt','w')\nimages = []\nnums = []\nfor num in range(1000):\n    filename = f\"predict_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    num=0\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                num=num+1\n    nums.append(num)\nfor i in range(len(nums)):\n    print(nums[i])\nfor num in range(1000):\n    filename = f\"precdict_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    for i in range(image.shape[0]): \n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                pz=i+0.5-image.shape[0]/2\n                py=(56/2/math.pi)*math.cos((j+0.5)/56*2*math.pi)\n                px=(56/2/math.pi)*math.sin((j+0.5)/56*2*math.pi)\n                pr=math.sqrt(px*px+py*py+pz*pz)\n                px=px/pr\n                py=py/pr\n                pz=pz/pr\n                print(f'{px} {py} {pz} {pixel_value}') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:30:12.878767Z","iopub.execute_input":"2025-02-01T01:30:12.879173Z","iopub.status.idle":"2025-02-01T01:30:44.310758Z","shell.execute_reply.started":"2025-02-01T01:30:12.879142Z","shell.execute_reply":"2025-02-01T01:30:44.310217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#无需运行，结果已记录\nimport os\nimport tifffile as tiff\nimport sys\nimport math\ndirectory_path = '/kaggle/input/gauss-s1-00-nl0-30-b0-50-jet'\nsys.stdout=open('/kaggle/working/output_truth.txt','w')\nimages = []\nnums = []\nfor num in range(1000):\n    filename = f\"truth_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    num=0\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                num=num+1\n    nums.append(num)\nfor i in range(len(nums)):\n    print(nums[i])\nfor num in range(1000):\n    filename = f\"truth_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    for i in range(image.shape[0]): \n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                pz=i+0.5-image.shape[0]/2\n                py=(56/2/math.pi)*math.cos((j+0.5)/56*2*math.pi)\n                px=(56/2/math.pi)*math.sin((j+0.5)/56*2*math.pi)\n                pr=math.sqrt(px*px+py*py+pz*pz)\n                px=px/pr\n                py=py/pr\n                pz=pz/pr\n                print(f'{px} {py} {pz} {pixel_value}') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:31:12.902373Z","iopub.execute_input":"2025-02-01T01:31:12.902698Z","iopub.status.idle":"2025-02-01T01:31:24.121267Z","shell.execute_reply.started":"2025-02-01T01:31:12.902676Z","shell.execute_reply":"2025-02-01T01:31:24.120570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#无需运行，结果已记录\nimport os\nimport tifffile as tiff\nimport sys\nimport math\ndirectory_path = '/kaggle/input/gauss-s1-00-nl0-30-b0-50-jet'\nsys.stdout=open('/kaggle/working/output_jet.txt','w')\nimages = []\nnums = []\nfor num in range(1000):\n    filename = f\"jet_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    num=0\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                num=num+1\n    nums.append(num)\nfor i in range(len(nums)):\n    print(nums[i])\nfor num in range(1000):\n    filename = f\"jet_{str(num)}.tiff\"\n    file_path = os.path.join(directory_path, filename)\n    image = tiff.imread(file_path)\n    images.append(image)\n    for i in range(image.shape[0]): \n        for j in range(image.shape[1]):\n            pixel_value = image[i, j]\n            if pixel_value>0:\n                pz=i+0.5-image.shape[0]/2\n                py=(56/2/math.pi)*math.cos((j+0.5)/56*2*math.pi)\n                px=(56/2/math.pi)*math.sin((j+0.5)/56*2*math.pi)\n                pr=math.sqrt(px*px+py*py+pz*pz)\n                px=px/pr\n                py=py/pr\n                pz=pz/pr\n                print(f'{px} {py} {pz} {pixel_value}') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T01:31:34.216740Z","iopub.execute_input":"2025-02-01T01:31:34.217027Z","iopub.status.idle":"2025-02-01T01:31:43.674548Z","shell.execute_reply.started":"2025-02-01T01:31:34.217006Z","shell.execute_reply":"2025-02-01T01:31:43.673784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"根据fastjet处理之后的result_predict.txt,result_truth.txt,result_jet.txt文件计算距离误差","metadata":{}},{"cell_type":"code","source":"#此处以CNN3D为例\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport math\nimport random\nfrom math import pi\nimport os\nimport sys\nsys.stdout=open('/kaggle/working/errors_CNN3D_jet_2dis.txt','w')\ndef chord_length(x1, y1, x2, y2, r):\n    L = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n    d = math.sqrt(r ** 2 - (L / 2) ** 2)\n    cos_theta = (L ** 2 + r ** 2 - d ** 2) / (2 * L * r)\n    theta = math.acos(cos_theta)\n    s = theta * r\n    return s\n    \ndef calculate_spatial_coordinates(rap, phi, Pt, E):\n    p_T = Pt \n    p_x = p_T * math.cos(phi)\n    p_y = p_T * math.sin(phi)\n    p_z = E * math.sinh(rap) \n    x = p_x / E \n    y = p_y / E \n    z = p_z / E \n    return x, y, z\n\ndef plot_filled_circle_in_3d(px=0, py=0, area=0, height=0, color=\"black\"):\n    theta = np.linspace(0, 2 * np.pi, 100) \n    radius= math.sqrt(area/2/pi)\n    ax.set_xlim(-3, 3) \n    ax.set_ylim(-1, 7)\n    ax.set_zlim(0, 200) \n    x = radius * np.cos(theta)+px \n    y = radius * np.sin(theta)+py  \n    z = np.zeros_like(x) \n    ax.plot(x, y, z, color=color)\n    for t in theta:\n        x_fill = np.linspace(-radius, radius, 50)\n        y_fill = np.sqrt(radius ** 2 - x_fill ** 2) \n        z_fill = np.zeros_like(x_fill)\n        ax.plot_surface(np.array([x_fill+px, x_fill+px]), np.array([y_fill+py, -y_fill+py]), np.array([z_fill, z_fill]),\n                        color=color, alpha=0.5)\n    x = 0.1*np.cos(theta)+px  \n    y = 0.1*np.sin(theta)+py  \n    z = np.linspace(0, height, 100) \n    X, Z = np.meshgrid(x, z)\n    Y = np.meshgrid(y, z)[0]\n    ax.plot_surface(X, Y, Z, color=color, alpha=0.8)\n    Z_top = np.full_like(X, z[-1])\n    Z_bottom = np.full_like(X, z[0])\n    ax.plot_surface(X, Y, Z_top, color=color, alpha=0.5)\n    ax.plot_surface(X, Y, Z_bottom, color=color, alpha=0.5)\n    ax.set_xlabel('y')\n    ax.set_ylabel('φ')\n    ax.set_zlabel('Pt(GeV)')\n\n# 打开文件\nwith open('kagge/input/result_truth.txt', 'r') as file:\n    # 读取文件内容\n    content = file.read()\n    # 分割内容为行\n    lines = content.strip().split('\\n')\n    # 读取每行的数字\n    truth_numbers = []\n    for line in lines:\n        # 分割行中的数字\n        truth_numbers_in_line = line.split()\n        # 将数字转换为浮点数列表\n        truth_numbers.extend(map(float, truth_numbers_in_line))\n    # 打印所有数字\n    #for number in numbers:\n        #print(number)\ntruth_numbers_size = len(truth_numbers)\n#print(truth_numbers_size)\nnum=0\ntruth_clusters= []\ntruth_raps= []\ntruth_phis= []\ntruth_pts= []\ntruth_E= []\ntruth_areas= []\ntruth_x= []\ntruth_y= []\ntruth_z= []\np=0\nfor i in range(truth_numbers_size):\n    if p!=0:\n        p=p-1\n        continue\n    if num==0:\n        truth_clusters.append(int(truth_numbers[i]))\n        num=int(truth_numbers[i])\n        continue\n    num=num-1;\n    truth_raps.append(truth_numbers[i])\n    truth_phis.append(truth_numbers[i+1])\n    truth_pts.append(truth_numbers[i+2])\n    truth_E.append(truth_numbers[i+3])\n    truth_areas.append(truth_numbers[i+4])\n    y, x, z = calculate_spatial_coordinates(truth_numbers[i], truth_numbers[i+1], truth_numbers[i+2],truth_numbers[i+3])\n    truth_y.append(y)\n    truth_x.append(x)\n    truth_z.append(z)\n    p=4\ntruth_clusters_size = len(truth_clusters)\n#print(truth_clusters_size)\ntruth_pos=0\ntruth_clusters[-1]=0\n\n\nwith open('kagge/input/result_jet.txt', 'r') as file:\n    # 读取文件内容\n    content = file.read()\n    # 分割内容为行\n    lines = content.strip().split('\\n')\n    # 读取每行的数字\n    jet_numbers = []\n    for line in lines:\n        # 分割行中的数字\n        jet_numbers_in_line = line.split()\n        # 将数字转换为浮点数列表\n        jet_numbers.extend(map(float, jet_numbers_in_line))\n    # 打印所有数字\n    #for number in numbers:\n        #print(number)\njet_numbers_size = len(jet_numbers)\n#print(jet_numbers_size)\nnum=0\njet_clusters= []\njet_raps= []\njet_phis= []\njet_pts= []\njet_E= []\njet_areas= []\njet_x= []\njet_y= []\njet_z= []\np=0\nfor i in range(jet_numbers_size):\n    if p!=0:\n        p=p-1\n        continue\n    if num==0:\n        jet_clusters.append(int(jet_numbers[i]))\n        num=int(jet_numbers[i])\n        continue\n    num=num-1;\n    jet_raps.append(jet_numbers[i])\n    jet_phis.append(jet_numbers[i+1])\n    jet_pts.append(jet_numbers[i+2])\n    jet_E.append(jet_numbers[i+3])\n    jet_areas.append(jet_numbers[i+4])\n    y, x, z = calculate_spatial_coordinates(jet_numbers[i], jet_numbers[i+1], jet_numbers[i+2],jet_numbers[i+3])\n    jet_y.append(y)\n    jet_x.append(x)\n    jet_z.append(z)\n    p=4\njet_clusters_size = len(jet_clusters)\n#print(jet_clusters_size)\njet_pos=0\njet_clusters[-1]=0\n\nwith open('kagge/input/result_predict.txt', 'r') as file:\n    # 读取文件内容\n    content = file.read()\n    # 分割内容为行\n    lines = content.strip().split('\\n')\n    # 读取每行的数字\n    predict_numbers = []\n    for line in lines:\n        # 分割行中的数字\n        predict_numbers_in_line = line.split()\n\n        # 将数字转换为浮点数列表\n        predict_numbers.extend(map(float, predict_numbers_in_line))\n\n    # 打印所有数字\n    #for number in numbers:\n        #print(number)\npredict_numbers_size = len(predict_numbers)\n#print(f'predict_numbers_size:{predict_numbers_size}')\nnum=0\npredict_clusters= []\npredict_raps= []\npredict_phis= []\npredict_pts= []\npredict_E= []\npredict_areas= []\npredict_x= []\npredict_y= []\npredict_z= []\np=0\nfor i in range(predict_numbers_size):\n    if p!=0:\n        p=p-1\n        continue\n    if num==0:\n        predict_clusters.append(int(predict_numbers[i]))\n        num=int(predict_numbers[i])\n        continue\n    num=num-1;\n    predict_raps.append(predict_numbers[i])\n    predict_phis.append(predict_numbers[i+1])\n    predict_pts.append(predict_numbers[i+2])\n    predict_E.append(predict_numbers[i+3])\n    predict_areas.append(predict_numbers[i+4])\n    y, x, z = calculate_spatial_coordinates(predict_numbers[i], predict_numbers[i+1], predict_numbers[i+2],predict_numbers[i+3])\n    predict_y.append(y)\n    predict_x.append(x)\n    predict_z.append(z)\n    p=4\npredict_clusters_size = len(predict_clusters)\n#print(predict_clusters_size)\npredict_pos=0\npredict_clusters[-1]=0\n\ndef distance(x1, y1, x2, y2):\n    \"\"\"Calculate the Euclidean distance between two points in 2D space.\"\"\"\n    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5\ndef cos_between_vectors(x1,y1,x2, y2):\n    # 计算点积\n    dot_product = x1 * x2 + y1 * y2\n    # 计算两个向量的模\n    magnitude_v1 = math.sqrt(x1**2 + y1**2)\n    magnitude_v2 = math.sqrt(x2**2 + y2**2)\n    # 计算夹角的余弦值\n    cos_angle = dot_product / (magnitude_v1 * magnitude_v2)\n    return cos_angle\n\noutput_folder='output_figure'\nfor i in range(truth_clusters_size):\n    truth_pos=truth_pos+truth_clusters[i-1]\n    jet_pos = jet_pos + jet_clusters[i - 1]\n    predict_pos = predict_pos + predict_clusters[i - 1]\n    if jet_clusters[i]==0:\n        continue\n    truth_min_cos =-1\n    predict_min_cos = -1\n    truth_min_distance = math.sqrt(56*56+56*56)\n    predict_min_distance = math.sqrt(56 * 56 + 56 * 56)\n    truth_id=0\n    for j in range(truth_clusters[i]):\n       if truth_min_distance>distance(x1=truth_raps[j+ truth_pos],y1=truth_phis[j+ truth_pos],x2=jet_raps[jet_pos],y2=jet_phis[jet_pos]):\n           truth_id=j\n           truth_min_distance = distance(x1=truth_raps[j + truth_pos], y1=truth_phis[j + truth_pos], x2=jet_raps[jet_pos],y2=jet_phis[jet_pos])\n    for j in range(predict_clusters[i]):\n       if predict_min_distance > distance(x1=predict_raps[j + predict_pos], y1=predict_phis[j + predict_pos],x2=jet_raps[jet_pos], y2=jet_phis[jet_pos]):\n           predict_id = j\n           predict_min_distance = distance(x1=predict_raps[j + predict_pos], y1=predict_phis[j + predict_pos],x2=jet_raps[jet_pos], y2=jet_phis[jet_pos])\n    x=jet_y[jet_pos]\n    y=jet_x[jet_pos]\n    z=jet_z[jet_pos]\n    r=56/2/math.pi\n    y=-y;\n    theta=math.atan(x/math.fabs(y))\n    dis=(math.pi-theta)*r\n    x=truth_y[truth_id+truth_pos]\n    y=truth_x[truth_id+truth_pos]\n    z=truth_z[truth_id+truth_pos]\n    r = 56 / 2 / math.pi\n    y = -y;\n    theta = math.atan(x / math.fabs(y))\n    dis = (math.pi - theta) * r\n    dis1=dis\n    z1=z\n    x1=x\n    y1=y\n    x=predict_y[predict_id+predict_pos]\n    y=predict_x[predict_id+predict_pos]\n    z=predict_z[predict_id+predict_pos]\n    r = 56 / 2 / math.pi\n    y = -y;\n    theta = math.atan(x / math.fabs(y))\n    dis = (math.pi - theta) * r\n    # print(f'x:{x*r/math.sqrt(x*x+y*y)}  y:{y*r/math.sqrt(x*x+y*y)} width:{z+28}  r:{x*x+y*y}');\n    #print(f'width:{dis}  height:{28 + z}');\n    print(f'{math.sqrt((dis1-dis)*(dis1-dis)+(z1-z)*(z1-z))}')\n   # print(f'{math.sqrt((x1 - x) * (x1 -x) + (y1-y)*(y1-y)+(z1 - z) * (z1 - z))}')\n   # print(f'{(predict_E[predict_id+predict_pos]-truth_E[truth_id+truth_pos])/truth_E[truth_id+truth_pos]}')\n    #print(f'{truth_pts[truth_id+truth_pos]}   {predict_pts[predict_id+predict_pos]}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}